<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: HBase | くまくまーZ]]></title>
  <link href="http://maiha.github.com/blog/categories/hbase/atom.xml" rel="self"/>
  <link href="http://maiha.github.com/"/>
  <updated>2012-04-03T05:28:16+09:00</updated>
  <id>http://maiha.github.com/</id>
  <author>
    <name><![CDATA[maiha]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[HBaseへデータ投入(importtsvの使い方)]]></title>
    <link href="http://maiha.github.com/blog/2012/04/02/hbase-import/"/>
    <updated>2012-04-02T12:31:00+09:00</updated>
    <id>http://maiha.github.com/blog/2012/04/02/hbase-import</id>
    <content type="html"><![CDATA[<h2>この記事の対象者</h2>

<ul>
<li>Hadoop/HBaseの初心者</li>
<li>HBaseが動く環境はできた</li>
<li>実験用のデータを入れたい</li>
<li>規模は100万か1億件ぐらい</li>
</ul>


<h2>インポート方法</h2>

<ul>
<li>a) hbase shell</li>
<li>b) importtsv</li>
<li>c) importtsv.bulk.output</li>
</ul>


<h2>a) hbase shell の使い方</h2>

<p><code>hbase shell</code> を使って登録する方法。rubyコードで実行できるので柔軟。</p>

<p><code>ruby
create 'sample', 'data'
('a'..'z').each {|i| put 'sample', i, 'data:alpha', i}
scan 'samples'
</code></p>

<pre><code>ROW                         COLUMN+CELL
 a                          column=data:alpha, timestamp=1333387516755, value=a
 b                          column=data:alpha, timestamp=1333387516772, value=b
...
</code></pre>

<p>手ごろなfixture作成にはよいが、これで1億件やる気はしない</p>

<h2>b) importtsv の使い方</h2>

<p>既存のインポートツールを利用する方法。
入力データはtsv,csvに限定されるが、<code>hbase.jar</code>に含まれる<code>ImportTsv</code>が目的に合致する。
実行には<code>hbase.jar</code>ファイルのフルパスが必要なので、まずはjarをlocateなどで探す。</p>

<pre><code>% locate /hbase-0. | grep .jar
/usr/local/lib/hbase-0.92.1/hbase-0.92.1-tests.jar
/usr/local/lib/hbase-0.92.1/hbase-0.92.1.jar
...
</code></pre>

<h3>hadoopでのjarファイルの実行</h3>

<p>あとは、以下の書式でjarファイル内の機能を実行できる。</p>

<pre><code>hadoop jar &lt;HBASE_JAR_FILE&gt; &lt;COMMAND&gt;
</code></pre>

<p><code>ImportTsv</code>のコマンド名は <code>importtsv</code> なので、以下のコマンドになる。</p>

<pre><code>% hadoop jar /usr/local/lib/hbase-0.92.1/hbase-0.92.1.jar importtsv
</code></pre>

<h3>HADOOP_CLASSPATH</h3>

<p>このとき、<code>HADOOP_CLASSPATH</code> が設定されていないと以下のようなエラーが出るので注意。</p>

<pre><code>% hadoop jar /usr/local/lib/hbase-0.92.1/hbase-0.92.1.jar importtsv
Exception in thread "main" java.lang.NoClassDefFoundError: com/google/common/collect/Multimap
    at org.apache.hadoop.hbase.mapreduce.Driver.main(Driver.java:43)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
</code></pre>

<p>その場合、例えばzshであれば、以下のコマンドで <code>HADOOP_CLASSPATH</code> を設定できる。</p>

<pre><code>export HADOOP_CLASSPATH=`hbase classpath`
</code></pre>

<h3>importtsvのヘルプ</h3>

<p>設定に問題がなければ、以下のようなヘルプが表示される。</p>

<pre><code>% hadoop jar /usr/local/lib/hbase-0.92.1/hbase-0.92.1.jar importtsv
ERROR: Wrong number of arguments: 0
Usage: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;
</code></pre>

<h3>引数: tablename</h3>

<p>データ投入先のHBaseテーブル名を指定する。
テーブルは、実行前に作っておく必要がある。
サンプルとして以下の<code>songs</code>テーブルを<code>hbase shell</code>から作成する。
CF名は何でもいいので適当に<code>data</code>にしておく。</p>

<pre><code>create 'songs', 'data'
</code></pre>

<h3>引数: inputdir</h3>

<p>データのソース(tsv,csv)を指定する。fileでなくdirとなっているのは、
<code>bulk.output</code>モードでは、ディレクトリを指定するからだと思う(多分)。
サンプルとして以下の<code>songs.csv</code>を利用する。</p>

<pre><code>20090805,ももいろパンチ,ももいろクローバー
20100505,怪盗少女,ももいろクローバー
20101110,ピンキージョーンズ,ももいろクローバー
20110706,Z伝説,ももいろクローバーZ
</code></pre>

<p>HBaseがローカル(Standalone)モードならば直接OS上のファイルを指定できるが、
分散モードであれば入力ファイルはhdfs上に存在している必要がある。</p>

<h3>hdfsへのファイル転送</h3>

<p>以下のコマンドで<code>songs.csv</code>をhdfs上に配置できる。引数はlocal、dstの順。面倒なので同名がよい。
コマンドは<code>dfs</code>と<code>fs</code>のどちらでもよい(多分同じ)。</p>

<pre><code>hadoop dfs -put songs.csv songs.csv
</code></pre>

<p>hdfs上のファイルは<code>-ls</code>で確認できる。</p>

<pre><code>% hadoop dfs -ls
-rw-r--r--   1 maiha supergroup         107 2012-04-02 03:14 /user/maiha/songs.csv
</code></pre>

<p>もしputでエラーが出た場合、
高確率で<code>core-site.xml</code>の<code>hadoop.tmp.dir</code>のパスに書き込み権限がでていないので、
以下のようにwritableにする。(PATHは環境依存)</p>

<pre><code>chmod a+rwxt /var/lib/hadoop/tmp
</code></pre>

<h3>引数: -Dimporttsv.columns=a,b,c</h3>

<p>csvのカラム名を指定する。
HTableのROW_KEYになる項目は<code>HBASE_ROW_KEY</code>という予約語を指定する。
それ以外は、対象テーブルのカラム名を<code>cf:qualifier</code>形式で指定する。</p>

<pre><code>20090805,ももいろパンチ,ももいろクローバー
</code></pre>

<p>の最初の日付をROW_KEYに、2つ目の曲名をdataのtitleに、
3つ目の歌手名をdataのsingerに入れたいので、
以下のような指定方法になる。</p>

<pre><code>-Dimporttsv.columns=HBASE_ROW_KEY,data:title,data:singer
</code></pre>

<p>入力ファイルの種別(csv,tsv)に関係なく、ここはカンマで区切る。
途中にスペースが入ってもいけないので(分かりづらいエラーが起きる)、
実行時は改行等にも注意する。</p>

<h3>引数: -Dimporttsv.separator=X</h3>

<p>tsvの場合は不要だが、ここではcsvを扱うので、セパレータを","として指定する。
ちなみに、セパレータには1バイトしか指定できない。</p>

<pre><code>-Dimporttsv.separator=,
</code></pre>

<h3>実行</h3>

<p>改めてまとめると、インポートのコマンドは以下のようになる。</p>

<pre><code>% hadoop jar /usr/local/lib/hbase-0.92.1/hbase-0.92.1.jar importtsv \
   -Dimporttsv.separator=, \
   -Dimporttsv.columns=HBASE_ROW_KEY,data:title,data:singer \
   songs songs.csv
</code></pre>

<h3>確認</h3>

<p><code>hbase shell</code>で確認。</p>

<pre><code>&gt; count 'songs'
4 row(s) in 0.0170 seconds

&gt; scan 'songs'
ROW               COLUMN+CELL
 20090805         column=data:singer, timestamp=1333393153254, value=...
</code></pre>

<h2>c) importtsv.bulk.output の使い方</h2>

<p>b)のimporttsvは、Reducerを使わずに直接HTableに随時書き込んでいく方式であるが、
ImportTsvには<code>importtsv.bulk.output</code>オプションによる別のインポート方式がある。
これは、以下の2コマンドによって確実でより高速なbulkインサートを提供する。</p>

<ol>
<li>importtsv: M/RによりHBaseの内部ストレージ形式の中間データをHFileとして作成</li>
<li>completebulkload: 作成したHFileを各クラスタに転送</li>
</ol>


<h3>c1: importtsv.bulk.output</h3>

<p>bulk.outputモードでimporttsvを実行するには、
<code>b) importtsv</code> の引数に <code>-Dimporttsv.bulk.output=XXX</code> を追加する。
ここで <code>XXX</code> には、中間データを保存するhdfsのディレクトリ(存在してはいけない)を指定する。
名前は何でもよいが、「テーブル名-bulk」が後から見たときにわかりやすい(消しやすい)。</p>

<h4>実行例</h4>

<p>具体的には、<code>b) importtsv</code> で実行したコマンド</p>

<pre><code>% hadoop jar /usr/local/lib/hbase-0.92.1/hbase-0.92.1.jar importtsv \
   -Dimporttsv.separator=, \
   -Dimporttsv.columns=HBASE_ROW_KEY,data:title,data:singer \
   songs songs.csv
</code></pre>

<p>に、<code>-Dimporttsv.bulk.output=songs-bulk</code> を追加して、bulk.outputを実行する。
もちろん、<code>b)</code>同様、入力ファイルはhdfsにputされている必要がある。
よって、bulk.outputモードでのimporttsvコマンドは以下のようになる。</p>

<pre><code>% hadoop jar /usr/local/lib/hbase-0.92.1/hbase-0.92.1.jar importtsv \
   -Dimporttsv.bulk.output=songs-bulk \
   -Dimporttsv.separator=, \
   -Dimporttsv.columns=HBASE_ROW_KEY,data:title,data:singer \
   songs songs.csv
</code></pre>

<h4>中間ファイルの確認</h4>

<p><code>b)</code> では直ちに songs テーブルに反映されたが、
<code>bulk.output</code> ではこの時点ではまだテーブルにwriteは発生していない。
変わりに、指定したパスに中間データが作成されている。</p>

<pre><code>% hadoop dfs -ls songs-bulk
Found 3 items
-rw-r--r--   1 maiha supergroup          0 2012-04-02 04:57 /user/maiha/songs-bulk/_SUCCESS
drwxr-xr-x   - maiha supergroup          0 2012-04-02 04:57 /user/maiha/songs-bulk/_logs
drwxr-xr-x   - maiha supergroup          0 2012-04-02 04:57 /user/maiha/songs-bulk/data
</code></pre>

<h3>c2: completebulkload</h3>

<pre><code>hadoop jar &lt;HBASE_JAR&gt; completebulkload &lt;HDFS_BULK_OUTPUT_PATH&gt; &lt;TABLE_NAME&gt;
</code></pre>

<p>中間データ(bulk.output)は、
<code>completebulkload</code>コマンドによって各クラスタの実テーブルに投入される。</p>

<pre><code>% hadoop jar /usr/local/lib/hbase-0.92.1/hbase-0.92.1.jar completebulkload songs-bulk songs
</code></pre>

<h3>注意点</h3>

<p>bulk.outputモードで作られる中間データファイルは、
最終的なHTableのサイズの4～5倍のサイズになるため、
十分なディスク容量が必要になる。</p>

<p>具体値として、5千万行(11GB)のcsvを入れた場合、
最終的なHTableのサイズは22GBだが、
中間ファイルの作成時に最大で100GBほどディスクを消費していた。
(環境はサーバ1の擬似分散の場合)</p>

<h2>事例: 郵便番号(12万件)</h2>

<p>この記事の想定規模を「100万か1億件」としておきながら、
実際に入れたのが4件というのは、いくらそれで理論的に大丈夫だと言われても釈然としないのも事実。
と言いつつも100万にも届かないが、ある程度の量の分かりやすい事例として
「郵便番号データ」を入れてみる。以下、手順のみで説明は省略。</p>

<h3>データ取得</h3>

<pre><code>wget http://www.post.japanpost.jp/zipcode/dl/kogaki/lzh/ken_all.lzh
lha e ken_all.lzh
nkf -Sud ken_all.csv | sed -e 's/[ "]//g' &gt; zips.csv
</code></pre>

<h3>データ確認</h3>

<pre><code>% wc -l zips.csv
123218 zips.csv
% head zips.csv
01101,060,0600000,ホッカイドウ,サッポロシチュウオウク,イカニケイサイガナイバアイ,北海道,札幌市中央区,以 下に掲載がない場合,0,0,0,0,0,0
01101,064,0640941,ホッカイドウ,サッポロシチュウオウク,アサヒガオカ,北海道,札幌市中央区,旭ケ丘,0,0,1,0,0,0
...
</code></pre>

<h3>hdfs転送</h3>

<pre><code>% hadoop dfs -put zips.csv zips.csv
</code></pre>

<h3>テーブル作成</h3>

<pre><code>create 'zips', 'd'
</code></pre>

<h3>データ投入</h3>

<pre><code>% hadoop jar /usr/local/lib/hbase-0.92.1/hbase-0.92.1.jar importtsv \
   -Dimporttsv.separator=, \
   -Dimporttsv.columns=d:c1,d:c2,HBASE_ROW_KEY,d:c4,d:c5,d:c6,d:c7,d:c8,d:c9,d:c10,d:c11,d:c12,d:c13,d:c14,d:c15 \
   zips zips.csv
</code></pre>

<h3>確認</h3>

<p>```scala
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.{HTable,Get}
import org.apache.hadoop.hbase.util.Bytes
import scala.collection.JavaConversions._     // for entrySet:java.util.Set</p>

<p>val conf  = new HBaseConfiguration
val songs = new HTable(conf, "zips")
val get   = new Get("9000005".getBytes)
val row   = songs.get(get)
val cf    = row.getFamilyMap("d".getBytes)</p>

<p>for (entry &lt;- cf.entrySet) {
  val key   = Bytes.toString(entry.getKey)
  val value = Bytes.toString(entry.getValue)
  printf("%s: %s\n", key, value)
}
```</p>

<pre><code>c1: 47201
c10: 0
c11: 0
c12: 0
c13: 0
c14: 0
c15: 0
c2: 900
c4: オキナワケン
c5: ナハシ
c6: アメク
c7: 沖縄県
c8: 那覇市
c9: 天久
</code></pre>

<h2>importtsvとbulk.outputはどちらがいいのか？</h2>

<ul>
<li>importtsvは中間ファイルも作らないので手軽</li>
<li>bulk.outputはHTableを作成して置換するイメージなので安心 (importの原子性が保障されている)</li>
</ul>


<p>importtsvはお手軽だが、落ちているリージョンサーバがあったりすると、
データの欠損や二重登録などの危険性がある。
データの正確性を考慮すると、bulk.outputを使うべき、という結論になる。</p>

<h2>ハマリ所</h2>

<ul>
<li>データファイル(tsv,csv)のカラム数とimporttsv.columnsの数が違うとエラー</li>
<li>bulk.outputを使う場合はディスク容量に注意 <a href="https://gist.github.com/2260678#comments">https://gist.github.com/2260678#comments</a></li>
</ul>


<h2>参考</h2>

<ul>
<li>importtsv <a href="http://archive.cloudera.com/cdh/3/hbase/bulk-loads.html">http://archive.cloudera.com/cdh/3/hbase/bulk-loads.html</a></li>
<li>日本郵便: 郵便番号データダウンロード <a href="http://www.post.japanpost.jp/zipcode/download.html">http://www.post.japanpost.jp/zipcode/download.html</a></li>
<li><a href="https://twitter.com/#!/ueshin/status/186779758951010304">https://twitter.com/#!/ueshin/status/186779758951010304</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ScalaでHbase (基本操作)]]></title>
    <link href="http://maiha.github.com/blog/2012/03/12/hbase-scala/"/>
    <updated>2012-03-12T13:20:00+09:00</updated>
    <id>http://maiha.github.com/blog/2012/03/12/hbase-scala</id>
    <content type="html"><![CDATA[<h3>versions</h3>

<ul>
<li>Scala-2.9.1.final</li>
<li>xsbt-0.11.2</li>
<li>Hbase-0.92.0</li>
<li>Hadoop-1.0.1</li>
</ul>


<h3>build.sbt</h3>

<p>```scala
resolvers += "Apache HBase" at "https://repository.apache.org/content/repositories/releases"</p>

<p>resolvers += "Thrift" at "http://people.apache.org/~rawson/repo/"</p>

<p>libraryDependencies ++= Seq(</p>

<pre><code>"org.apache.hadoop" % "hadoop-core" % "1.0.1",
"org.apache.hbase" % "hbase" % "0.92.0"
</code></pre>

<p>)
```</p>

<h3>使用例 (共通部分)</h3>

<p>```scala
import org.apache.hadoop.hbase.{HBaseConfiguration,HTableDescriptor,HColumnDescriptor}
import org.apache.hadoop.hbase.client.{HBaseAdmin,HTable,Put,Get,Delete,Scan}
import org.apache.hadoop.hbase.util.Bytes</p>

<p>val conf  = new HBaseConfiguration
val admin = new HBaseAdmin(conf)
```</p>

<h3>既存テーブルの情報を取得</h3>

<p><code>hbase shell</code>の<code>list</code></p>

<p><code>scala
admin.listTables.foreach(println)
</code></p>

<pre><code>{NAME =&gt; 'adviewlogs', FAMILIES =&gt; [{NAME =&gt; 'ad', ...
{NAME =&gt; 'users', FAMILIES =&gt; [{NAME =&gt; 'name', ...
</code></pre>

<h3>テーブルの新規作成</h3>

<p><code>hbase shell</code>の<code>create 'songs', 'data'</code></p>

<p><code>scala
val schema = new HTableDescriptor("songs")
schema.addFamily(new HColumnDescriptor("data"))
admin.createTable(schema)
</code></p>

<h3>行の追加</h3>

<p><code>hbase shell</code>の<code>put 'songs', 'row1', 'data:title', '走れ'</code></p>

<p>```scala
val songs = new HTable(conf, "songs")
val put   = new Put("row1".getBytes)
put.add("data".getBytes, "title".getBytes, "走れ".getBytes)
songs.put(put)</p>

<p>```</p>

<h3>行の追加(複数のcf)</h3>

<p><code>hbase shell</code>の<code>put 'songs', 'row2', 'data', {'title'=&gt;'怪盗少女', 'singer'=&gt;'ももクロ'}</code></p>

<p>```scala
val songs = new HTable(conf, "songs")
val put   = new Put("row2".getBytes)
put.add("data".getBytes, "title".getBytes, "怪盗少女".getBytes)
put.add("data".getBytes, "singer".getBytes, "ももクロ".getBytes)
songs.put(put)</p>

<p>```</p>

<h3>行の取得 (cf:qualifier指定)</h3>

<p><code>hbase shell</code>の<code>get 'songs', 'row2', 'data:title'</code></p>

<p>```scala
val songs  = new HTable(conf, "songs")
val get    = new Get("row2".getBytes)
val row2   = songs.get(get)
val title  = row2.getValue("data".getBytes, "title".getBytes)
println(Bytes.toString(title))</p>

<p>```</p>

<pre><code>怪盗少女
</code></pre>

<h3>行の取得 (cf内全部)</h3>

<p><code>hbase shell</code>の<code>get 'songs', 'row2', 'data'</code></p>

<p>```scala
import scala.collection.JavaConversions._     // for entrySet:java.util.Set</p>

<p>val songs  = new HTable(conf, "songs")
val get    = new Get("row2".getBytes)
val row2   = songs.get(get)
val cf     = row2.getFamilyMap("data".getBytes)</p>

<p>for (entry &lt;- cf.entrySet) {
  val key   = Bytes.toString(entry.getKey)
  val value = Bytes.toString(entry.getValue)
  printf("%s: %s\n", key, value)
}
```</p>

<pre><code>singer: ももクロ
title: 怪盗少女
</code></pre>

<p>存在しない場合は <code>getFamilyMap</code>が<code>null</code> (要チェック)</p>

<h3>行の削除 (cf指定)</h3>

<p><code>hbase shell</code>の<code>delete 'songs', 'row1', 'data'</code> (shellではqualifierがないと動かない?)</p>

<p>```scala
val songs  = new HTable(conf, "songs")
val del    = new Delete("row1".getBytes)
del.deleteFamily("data".getBytes)
songs.delete(del)</p>

<p>```
存在しない場合はnop</p>

<h3>行の削除 (1行全体)</h3>

<p><code>hbase shell</code>の<code>delete 'songs', 'row2'</code>みたいなこと (これもshellではできない)</p>

<p>```scala
val songs  = new HTable(conf, "songs")
val del    = new Delete("row2".getBytes)
songs.delete(del)</p>

<p>```
存在しない場合はnop</p>

<h3>件数取得</h3>

<p><code>hbase shell</code>の<code>count 'songs'</code></p>

<p>自分でscanするしかない？
<code>org.apache.hadoop.hbase.mapreduce.RowCounter</code> あたりを使う？</p>

<h3>行の存在確認</h3>

<p><code>hbase shell</code>にはない</p>

<p>```scala
val songs  = new HTable(conf, "songs")
val get    = new Get("row3".getBytes)
songs.exists(get)</p>

<p>```</p>

<h3>範囲取得</h3>

<p>検索用データとして以下を <code>hbase shell</code> で追加 (ROW_KEY: 発売日)
<code>ruby
put 'songs', '20090805', 'data:title', 'ももいろパンチ'
put 'songs', '20100505', 'data:title', '怪盗少女'
put 'songs', '20101110', 'data:title', 'ピンキージョーンズ'
put 'songs', '20110706', 'data:title', 'Z伝説'
</code></p>

<p>2010年に発売された曲を調べる</p>

<p><code>scala
val scan = new Scan("2010".getBytes, "2011".getBytes)
val iter = songs.getScanner(scan)
println(iter.size)
</code></p>

<pre><code>2
</code></pre>

<ul>
<li>Scan(start, end)のendは含まれない (ruby の<code>start...end</code>)</li>
<li>HbaseではRAW_KEYによる文字列ソートが保証されている (RAW_KEYの設計が重要)</li>
</ul>


<h3>テーブル削除</h3>

<p><code>hbase shell</code>の<code>disable 'songs'</code>と<code>delete</code>songs'`</p>

<p><code>scala
admin.disableTable("songs".getBytes)
admin.deleteTable("songs".getBytes)
</code></p>

<h3>参考</h3>

<ul>
<li><a href="http://wiki.apache.org/hadoop/Hbase/Scala">http://wiki.apache.org/hadoop/Hbase/Scala</a></li>
<li><a href="http://www.atmarkit.co.jp/fjava/rensai4/bigdata_java03/03.html">http://www.atmarkit.co.jp/fjava/rensai4/bigdata_java03/03.html</a></li>
<li>ScalaからHBaseを使ってみる（0.20.6）<a href="http://www.mwsoft.jp/programming/hadoop/hbase_scala.html">http://www.mwsoft.jp/programming/hadoop/hbase_scala.html</a></li>
<li><a href="http://happy-camper.st/lang/java/hbase/hbase-mapreduce-in-scala.html">http://happy-camper.st/lang/java/hbase/hbase-mapreduce-in-scala.html</a></li>
<li>RowCounter <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/RowCounter.html">http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/RowCounter.html</a></li>
<li>ももいろクローバーZ <a href="http://ja.wikipedia.org/wiki/%E3%82%82%E3%82%82%E3%81%84%E3%82%8D%E3%82%AF%E3%83%AD%E3%83%BC%E3%83%90%E3%83%BCZ">http://ja.wikipedia.org/wiki/%E3%82%82%E3%82%82%E3%81%84%E3%82%8D%E3%82%AF%E3%83%AD%E3%83%BC%E3%83%90%E3%83%BCZ</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hbaseを動かしてみる]]></title>
    <link href="http://maiha.github.com/blog/2012/03/09/hbase/"/>
    <updated>2012-03-09T10:40:00+09:00</updated>
    <id>http://maiha.github.com/blog/2012/03/09/hbase</id>
    <content type="html"><![CDATA[<p>Hbaseを使ったことがない Ubuntu 11.10(oneiric) ユーザがHbaseを動かすまでのメモ</p>

<h3>インストール</h3>

<p>正式なパッケージはないのでppaを利用する。
(パッケージはまだ実験中でfeedback歓迎らしい [2012.3.7現在])</p>

<pre><code>sudo apt-get install python-software-properties
sudo add-apt-repository ppa:hadoop-ubuntu/dev
sudo apt-get update
sudo apt-get install hbase
</code></pre>

<p><code>/etc/hosts</code> を見て、loopback IPが<code>127.0.0.1</code>になってるいるかを確認する
(内部的に127.0.0.1決め打ちでアクセス？)</p>

<pre><code>127.0.0.1 localhost
</code></pre>

<p><code>/etc/security/limits.conf</code> の最後に以下を追加
("hbase" は動かすユーザ名を指定)</p>

<pre><code>hbase  -       nofile  32768
</code></pre>

<p><code>/etc/pam.d/common-session</code> の最後に以下を追加
(前述のlimits.confを有効にするための設定)</p>

<pre><code>session required  pam_limits.so
</code></pre>

<h3>設定</h3>

<p>データファイルの保存場所を作成。(defaultは/tmp)</p>

<pre><code>mkdir /var/lib/hbase/data
chown hbase:hbase /var/lib/hbase/data
</code></pre>

<p><code>/etc/hbase/conf/hbase-site.xml</code> の <code>configuration</code> の中に以下を追加</p>

<p>```xml
<configuration>
  <property></p>

<pre><code>&lt;name&gt;hbase.rootdir&lt;/name&gt;
&lt;value&gt;file:////var/lib/hbase/data&lt;/value&gt;
</code></pre>

<p>  </property>
</configuration>
```</p>

<h3>サービスの管理 (hbaseユーザで実行)</h3>

<p><code>hbase master</code> で管理できる。引数なしでusageが表示</p>

<pre><code>% hbase master
Usage: Master [opts] start|stop
 start  Start Master. If local mode, start Master and RegionServer in same JVM
 stop   Start cluster shutdown; Master signals RegionServer shutdown
 where [opts] are:
   --minServers=&lt;servers&gt;    Minimum RegionServers needed to host user tables.
   --backup                  Master should start in backup mode
</code></pre>

<h4>サービス起動</h4>

<p>startはforegroundで動くので、"> log &amp;" とかがいいのかもしれない。</p>

<pre><code>hbase master start &gt; server.log 2&gt;&amp;1 &amp;
</code></pre>

<h4>サービス停止</h4>

<pre><code>hbase master stop
</code></pre>

<h3>クライアントから利用</h3>

<p><code>hbase shell</code> を実行するとconsoleが開く。見た目はirbで、中身もirb。helpでヘルプが見える。
一般ユーザが認証なしで接続できる。
(TODO: 認証方法)</p>

<pre><code>% hbase shell
hbase(main):001:0&gt; self.class.ancestors
=&gt; [Object, HBaseConstants, Java, Kernel]
hbase(main):002:0&gt; help
HBase Shell, version 0.92.0, r1231986, Mon Jan 16 13:16:35 UTC 2012
Type 'help "COMMAND"', (e.g. 'help "get"' -- the quotes are necessary) for help on a specific command.
Commands are grouped. Type 'help "COMMAND_GROUP"', (e.g. 'help "general"') for help on a command group.
...
</code></pre>

<h4>コマンド一覧</h4>

<pre><code>テーブル作成　create 't1', 'f1', 'f2',...
テーブル削除　drop 't1'
テーブル有効　enable 't1'
テーブル無効　disable 't1'
テーブル情報　describe 't1'
データ登録　　put 't1', 'r1', 'c1', 'value', ts1
データ削除　  delete 't1', 'r1', 'c1', ts1
全行表示　　　scan 't1'
一行表示　　　get 't1', 'r1'
</code></pre>

<h4>実行例</h4>

<p><code>ruby
hbase(main):001:0&gt; create 'users', 'name', 'color'
hbase(main):002:0&gt; put 'users', '1', 'name:name', '百田夏菜子'
hbase(main):003:0&gt; put 'users', '1', 'name:yomi', 'ももたかなこ'
hbase(main):004:0&gt; put 'users', '1', 'color:', 'red'
hbase(main):005:0&gt; put 'users', '2', 'name:name', 'hoge'
hbase(main):006:0&gt; get 'users', '1', ['name:yomi', 'color']
COLUMN                          CELL
 color:                         timestamp=1331280696571, value=red
 name:yomi                      timestamp=1331280681186, value=\xE3\x82\x82\xE3 ...
hbase(main):007:0&gt; delete 'users', '2', 'name:name'
hbase(main):008:0&gt; drop 'users'
ERROR: Table users is enabled. Disable it first.
hbase(main):009:0&gt; disable 'users'
hbase(main):010:0&gt; drop 'users'
</code></p>

<p>(TODO: 日本語の表示について調べる)</p>

<h3>参考</h3>

<ul>
<li>Install hbase on Ubuntu 11.10  <a href="http://www.mail-archive.com/user@hbase.apache.org/msg14752.html">http://www.mail-archive.com/user@hbase.apache.org/msg14752.html</a></li>
<li>Apache Hbase <a href="http://hbase.apache.org/book.html#quickstart">http://hbase.apache.org/book.html#quickstart</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
